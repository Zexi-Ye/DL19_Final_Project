{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T11:32:11.676587Z",
     "start_time": "2019-04-25T11:32:05.745333Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "# import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T11:32:23.314376Z",
     "start_time": "2019-04-25T11:32:23.195882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T11:51:12.946512Z",
     "start_time": "2019-04-25T11:51:12.936139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T11:54:05.135980Z",
     "start_time": "2019-04-25T11:54:05.130198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many devices are around\n",
    "torch.cuda.device_count()\n",
    "# Set it to a particular device\n",
    "torch.cuda.set_device(1)\n",
    "# Check which device you are on\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path='/scratch/by783/DL_Final_models/'+'190424_vgg_ae'#args.save\n",
    "model_name = 'vgg'#args.model\n",
    "num_epochs = 10 #args.epochs\n",
    "feature_extract = True # str2bool(args.pretrained)\n",
    "\n",
    "###################### fixed_params ###################################\n",
    "\n",
    "num_classes = 1000\n",
    "loader_image_path='/scratch/by783/DL_Final/ssl_data_96'\n",
    "loader_batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(path, batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            #transforms.Resize(input_size),\n",
    "            #transforms.CenterCrop(input_size),\n",
    "            # use model fitted with the image size, so no need to resize\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            # https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "            # [mean],[std] for different channels\n",
    "        ]\n",
    "    )\n",
    "    sup_train_data = datasets.ImageFolder('{}/{}/train'.format(path, 'supervised'), transform=transform)\n",
    "    sup_val_data = datasets.ImageFolder('{}/{}/val'.format(path, 'supervised'), transform=transform)\n",
    "    unsup_data = datasets.ImageFolder('{}/{}/'.format(path, 'unsupervised'), transform=transform)\n",
    "    # source code: https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
    "    # Main idea:\n",
    "    data_loader_sup_train = torch.utils.data.DataLoader(\n",
    "        sup_train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_sup_val = torch.utils.data.DataLoader(\n",
    "        sup_val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_unsup = torch.utils.data.DataLoader(\n",
    "        unsup_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print('sup_train_data.class_to_idx==sup_val_data.class_to_idx: ',\n",
    "          sup_train_data.class_to_idx == sup_val_data.class_to_idx)\n",
    "\n",
    "    return data_loader_sup_train, data_loader_sup_val, data_loader_unsup, sup_train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "\n",
    "\n",
    "    if model_name != \"vgg\":\n",
    "        sys.stdout.write('We only have vgg now!!!')\n",
    "    else:\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "        ##########\n",
    "\n",
    "        model_ft.avgpool = nn.AdaptiveAvgPool2d(output_size=(3, 3))\n",
    "\n",
    "        model_ft.classifier[0] = nn.Linear(in_features=4608, out_features=4096, bias=True)\n",
    "        model_ft.classifier[3] = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        model_ft.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "\n",
    "\n",
    "        input_size = 96\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained=True\n",
    "model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "set_parameter_requires_grad(model_ft, feature_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace)\n",
       "  (5): Dropout(p=0.5)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        ### ENCODER\n",
    "        \n",
    "        # 28x28x1 => 28x28x4\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      # (1(28-1) - 28 + 3) / 2 = 1\n",
    "                                      padding=1) \n",
    "        # 28x28x4 => 14x14x4                              \n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         # (2(14-1) - 28 + 2) / 2 = 0\n",
    "                                         padding=0)                                       \n",
    "        # 14x14x4 => 14x14x8\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      # (1(14-1) - 14 + 3) / 2 = 1\n",
    "                                      padding=1)                 \n",
    "        # 14x14x8 => 7x7x8                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         # (2(7-1) - 14 + 2) / 2 = 0\n",
    "                                         padding=0)\n",
    "        \n",
    "        ### DECODER\n",
    "                                         \n",
    "        # 7x7x8 => 15x15x4                          \n",
    "        self.deconv_1 = torch.nn.ConvTranspose2d(in_channels=8,\n",
    "                                                 out_channels=4,\n",
    "                                                 kernel_size=(3, 3),\n",
    "                                                 stride=(2, 2),\n",
    "                                                 padding=0)\n",
    "        \n",
    "        # 15x15x4  => 31x31x1                           \n",
    "        self.deconv_2 = torch.nn.ConvTranspose2d(in_channels=4,\n",
    "                                                 out_channels=1,\n",
    "                                                 kernel_size=(3, 3),\n",
    "                                                 stride=(2, 2),\n",
    "                                                 padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### ENCODER\n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool_2(x)\n",
    "        \n",
    "        ### DECODER\n",
    "        x = self.deconv_1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.deconv_2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        logits = x[:, :, 2:30, 2:30]\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(1)\n",
    "model_try = ConvolutionalAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalAutoencoder(\n",
       "  (conv_1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool_1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool_2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (deconv_1): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (deconv_2): ConvTranspose2d(4, 1, kernel_size=(3, 3), stride=(2, 2))\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace)\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU(inplace)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ReLU(inplace)\n",
       "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace)\n",
       "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (27): ReLU(inplace)\n",
       "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/37837682/python-class-input-argument/37837766\n",
    "# https://github.com/awentzonline/pytorch-cns/blob/master/examples/vggmse.py\n",
    "\n",
    "class Model_Based_Autoencoder(torch.nn.Module):\n",
    "    def __init__(self,model_name):\n",
    "        super(Model_Based_Autoencoder, self).__init__()\n",
    "        if model_name!='vgg':\n",
    "            sys.stdout.write('Dear, we only support vgg now...')\n",
    "        \n",
    "        self.encoder = models.vgg11_bn(pretrained=use_pretrained).features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,512,kernel_size=(2, 2), stride=(0, 0), padding=(2, 2)),#de-conv8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512,512,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),#de-conv7\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512,512,kernel_size=(2, 2), stride=(0, 0), padding=(2, 2)),#de-conv6\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512,256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),#de-conv5\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256,256,kernel_size=(2, 2), stride=(0, 0), padding=(2, 2)),#de-conv4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256,128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),#de-conv3\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128,64,kernel_size=(2, 2), stride=(0, 0), padding=(2, 2)),#de-conv2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64,3,kernel_size=(2, 2), stride=(0, 0), padding=(2, 2)),#de-conv1\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:55:16.674851Z",
     "start_time": "2019-04-25T12:55:16.663028Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model_Based_Autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f7fe18bec9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_vggae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel_Based_Autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Model_Based_Autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "model_vggae=Model_Based_Autoencoder('vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(model_vggae.encoder, feature_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vggae.encoder[0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vggae.decoder[0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate=0.001\n",
    "optimizer = torch.optim.Adam(model_vggae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './dc_img/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './conv_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:29:14.340613Z",
     "start_time": "2019-04-25T12:29:14.335391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:30:49.689312Z",
     "start_time": "2019-04-25T12:30:49.683955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T12:53:39.542770Z",
     "start_time": "2019-04-25T12:53:39.489860Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    #unsupervised learning, we do not need train and vals\n",
    "    since = time.time()\n",
    "    loss_history=[]\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epoches):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        ################# train the model on unsupervised data ############\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, _ in dataloaders['unlabeled']:\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloaders['unlabeled'].dataset)\n",
    "        sys.stdout.write('Training time: {:.0f}s'.format( time.time() - since ))\n",
    "        \n",
    "        ################# evaluate the model performance on labeled data ############\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        eval_loss=0.0\n",
    "        for inputs, _ in dataloaders['labeled']:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            eval_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        epoch_eval_loss = running_loss / len(dataloaders['labeled'].dataset)\n",
    "        sys.stdout.write('Evaluation time: {:.0f}s'.format( time.time() - since ))        \n",
    "        sys.stdout.write('Training loss: {:.4f} Eval loss: {:.4f}'.format(epoch_loss, epoch_eval_loss))\n",
    "        \n",
    "        #################\n",
    "        \n",
    "        loss_history.append( ( epoch_loss.item(),epoch_eval_loss.item() ) )\n",
    "        \n",
    "        \n",
    "        if epoch_eval_loss < best_loss:\n",
    "            best_loss = epoch_eval_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            with open(save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            \n",
    "        with open(save_path+'_val_acc', 'w') as f:\n",
    "            for item in loss_history:\n",
    "                f.write(\"unlabeled: %s, labeled: s% \\n,  \" % (item[0],item[1]) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return model, loss_history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
