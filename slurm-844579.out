begin_raw_alexnet_model
Downloading: "https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth" to /home/by783/.torch/models/alexnet-owt-4df8aa71.pth
  0%|          | 0/244418560 [00:00<?, ?it/s]  1%|          | 2211840/244418560 [00:00<00:12, 19737145.31it/s]  1%|          | 2809856/244418560 [00:00<00:38, 6311044.73it/s]   2%|▏         | 4505600/244418560 [00:00<00:30, 7775410.96it/s]  2%|▏         | 5455872/244418560 [00:00<00:29, 8200336.25it/s]  3%|▎         | 6619136/244418560 [00:00<00:26, 8887966.35it/s]  3%|▎         | 8028160/244418560 [00:00<00:23, 9886112.20it/s]  4%|▍         | 9650176/244418560 [00:00<00:20, 11196811.12it/s]  5%|▍         | 11411456/244418560 [00:00<00:18, 12567624.61it/s]  5%|▌         | 13336576/244418560 [00:01<00:16, 13989836.14it/s]  6%|▋         | 15638528/244418560 [00:01<00:14, 15732279.62it/s]  7%|▋         | 18006016/244418560 [00:01<00:12, 17453970.39it/s]  8%|▊         | 20578304/244418560 [00:01<00:11, 19284387.97it/s]  9%|▉         | 23085056/244418560 [00:01<00:10, 20556732.03it/s] 11%|█         | 25886720/244418560 [00:01<00:09, 22207533.86it/s] 12%|█▏        | 29212672/244418560 [00:01<00:08, 24495122.12it/s] 13%|█▎        | 32555008/244418560 [00:01<00:07, 26578824.08it/s] 15%|█▍        | 35889152/244418560 [00:01<00:07, 28298929.28it/s] 16%|█▌        | 39313408/244418560 [00:01<00:06, 29852991.79it/s] 18%|█▊        | 42967040/244418560 [00:02<00:06, 31582941.70it/s] 19%|█▉        | 46587904/244418560 [00:02<00:06, 32833458.89it/s] 21%|██        | 50700288/244418560 [00:02<00:05, 34941408.29it/s] 22%|██▏       | 54763520/244418560 [00:02<00:05, 36472023.03it/s] 24%|██▍       | 58941440/244418560 [00:02<00:04, 37914762.38it/s] 26%|██▌       | 62816256/244418560 [00:02<00:05, 32555699.89it/s] 28%|██▊       | 68042752/244418560 [00:02<00:04, 36705351.26it/s] 30%|██▉       | 72802304/244418560 [00:02<00:04, 39158669.87it/s] 32%|███▏      | 77832192/244418560 [00:02<00:03, 41938795.22it/s] 34%|███▍      | 82640896/244418560 [00:03<00:03, 43180004.68it/s] 36%|███▌      | 87293952/244418560 [00:03<00:03, 44083186.07it/s] 38%|███▊      | 91832320/244418560 [00:03<00:03, 43271329.31it/s] 40%|████      | 98459648/244418560 [00:03<00:03, 48293168.96it/s] 43%|████▎     | 104153088/244418560 [00:03<00:02, 50592168.34it/s] 45%|████▌     | 110608384/244418560 [00:03<00:02, 54096832.11it/s] 48%|████▊     | 117137408/244418560 [00:03<00:02, 57027817.85it/s] 50%|█████     | 123035648/244418560 [00:03<00:03, 38327031.97it/s] 52%|█████▏    | 127819776/244418560 [00:04<00:04, 26997721.21it/s] 54%|█████▍    | 131645440/244418560 [00:04<00:04, 24605785.55it/s] 55%|█████▌    | 134938624/244418560 [00:04<00:06, 17868981.35it/s] 58%|█████▊    | 141533184/244418560 [00:04<00:04, 22869589.03it/s] 61%|██████    | 148144128/244418560 [00:04<00:03, 28446068.15it/s] 63%|██████▎   | 155058176/244418560 [00:05<00:02, 34543228.10it/s] 66%|██████▌   | 160391168/244418560 [00:05<00:02, 30834001.79it/s] 70%|██████▉   | 170598400/244418560 [00:05<00:01, 38996811.35it/s] 73%|███████▎  | 178487296/244418560 [00:05<00:01, 45967940.63it/s] 76%|███████▌  | 185188352/244418560 [00:05<00:01, 49049085.60it/s] 79%|███████▉  | 193937408/244418560 [00:05<00:00, 56492464.13it/s] 82%|████████▏ | 201367552/244418560 [00:05<00:00, 60862299.80it/s] 86%|████████▌ | 209313792/244418560 [00:05<00:00, 65451974.39it/s] 89%|████████▉ | 218144768/244418560 [00:05<00:00, 70598184.57it/s] 93%|█████████▎| 226467840/244418560 [00:06<00:00, 73956573.17it/s] 96%|█████████▌| 234717184/244418560 [00:06<00:00, 76319974.19it/s] 99%|█████████▉| 243056640/244418560 [00:06<00:00, 78302002.30it/s]100%|██████████| 244418560/244418560 [00:06<00:00, 38946529.70it/s]
PyTorch Version:  0.4.1
Torchvision Version:  0.2.2
GPU mode
sup_train_data.class_to_idx==sup_val_data.class_to_idx:  True
Params to learn:
	 classifier.6.weight
	 classifier.6.bias
Begin to train
Epoch 0/19
----------
train Loss: 5.0075 Acc: 0.1534
val Loss: 3.8171 Acc: 0.2700

Epoch 1/19
----------
train Loss: 3.3673 Acc: 0.3186
val Loss: 3.3386 Acc: 0.3107

Epoch 2/19
----------
train Loss: 2.9033 Acc: 0.3783
val Loss: 3.1757 Acc: 0.3267

Epoch 3/19
----------
train Loss: 2.6291 Acc: 0.4215
val Loss: 3.0868 Acc: 0.3372

Epoch 4/19
----------
train Loss: 2.4298 Acc: 0.4539
val Loss: 3.0328 Acc: 0.3419

Epoch 5/19
----------
train Loss: 2.2704 Acc: 0.4837
val Loss: 2.9987 Acc: 0.3465

Epoch 6/19
----------
train Loss: 2.1300 Acc: 0.5121
val Loss: 2.9723 Acc: 0.3492

Epoch 7/19
----------
train Loss: 2.0141 Acc: 0.5337
val Loss: 2.9588 Acc: 0.3509

Epoch 8/19
----------
train Loss: 1.9029 Acc: 0.5571
val Loss: 2.9489 Acc: 0.3536

Epoch 9/19
----------
train Loss: 1.8164 Acc: 0.5776
val Loss: 2.9397 Acc: 0.3542

Epoch 10/19
----------
train Loss: 1.7302 Acc: 0.5963
val Loss: 2.9332 Acc: 0.3580

Epoch 11/19
----------
train Loss: 1.6536 Acc: 0.6139
val Loss: 2.9323 Acc: 0.3573

Epoch 12/19
----------
train Loss: 1.5774 Acc: 0.6336
val Loss: 2.9316 Acc: 0.3573

Epoch 13/19
----------
train Loss: 1.5142 Acc: 0.6478
val Loss: 2.9329 Acc: 0.3570

Epoch 14/19
----------
train Loss: 1.4479 Acc: 0.6647
val Loss: 2.9316 Acc: 0.3599

Epoch 15/19
----------
train Loss: 1.3942 Acc: 0.6794
val Loss: 2.9365 Acc: 0.3601

Epoch 16/19
----------
train Loss: 1.3448 Acc: 0.6912
val Loss: 2.9381 Acc: 0.3584

Epoch 17/19
----------
train Loss: 1.2908 Acc: 0.7049
val Loss: 2.9424 Acc: 0.3592

Epoch 18/19
----------
train Loss: 1.2452 Acc: 0.7176
val Loss: 2.9480 Acc: 0.3594

Epoch 19/19
----------
train Loss: 1.2021 Acc: 0.7292
val Loss: 2.9512 Acc: 0.3588

Training complete in 1297m 24s
Best val Acc: 0.360125
slurmstepd: error: The SLURM utilizes Linux cgroups for resource containment. For reference please read the explanation in the bug report: https://bugs.schedmd.com/show_bug.cgi?id=3214#c4
slurmstepd: error: The SLURM utilizes Linux cgroups for resource containment. For reference please read the explanation in the bug report: https://bugs.schedmd.com/show_bug.cgi?id=3214#c4
