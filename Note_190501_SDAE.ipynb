{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hook 无法parallel。。。需要注意的东西多的一匹。。。不确定parallel，至少写起来真恶心，pass。。。底层的东西简单但是也蠢啊，呃呃呃呃呃呃呃呃呃。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.179129Z",
     "start_time": "2019-05-02T12:21:06.501293Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.208776Z",
     "start_time": "2019-05-02T12:21:07.181643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--noise-level'], dest='noise_level', nargs=None, const=None, default=0.3, type=<class 'float'>, choices=None, help='add noise to input', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################   set args  #######################\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DL19_FinalProject_PyTorch')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='vgg',\n",
    "                    help='type of cnn (\"resnet\", \"alexnet\",\"vgg\",\"squeezenet\",\"densenet\",\"inception\")')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--epochs', type=int, default=25,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument(\"--pretrained\", type=str, default='True',\n",
    "                    help=\"use pre-trained conv layers.\")\n",
    "parser.add_argument('--noise-level', type=float, default=0.3,\n",
    "                    help='add noise to input')\n",
    "\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.214960Z",
     "start_time": "2019-05-02T12:21:07.211364Z"
    }
   },
   "outputs": [],
   "source": [
    "args=parser.parse_args('--model vgg --batch-size 256 --save 190501_try.pt --epochs 40 --pretrained False --noise-level 0.3'.split())\n",
    "\n",
    "#16GB 2GPU 512 ok\n",
    "# 768 out of memory\n",
    "\n",
    "\n",
    "# 256 14batch problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.223162Z",
     "start_time": "2019-05-02T12:21:07.217721Z"
    }
   },
   "outputs": [],
   "source": [
    "####################### input params ##################################\n",
    "save_path='/scratch/by783/DL_Final_models/'+args.save\n",
    "model_name = args.model\n",
    "num_epochs = args.epochs\n",
    "pin_encoder = str2bool(args.pretrained)\n",
    "loader_batch_size=args.batch_size\n",
    "noise_level = args.noise_level\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "###################### fixed_params ###################################\n",
    "num_classes = 1000\n",
    "loader_image_path='/scratch/by783/DL_Final/ssl_data_96'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.230208Z",
     "start_time": "2019-05-02T12:21:07.225687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.336468Z",
     "start_time": "2019-05-02T12:21:07.232645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 0.4.1\n",
      "Torchvision Version: 0.2.2\n",
      " GPU mode \n"
     ]
    }
   ],
   "source": [
    "sys.stdout.write(\"PyTorch Version: {}\\n\".format(torch.__version__))\n",
    "sys.stdout.write(\"Torchvision Version: {}\\n \".format(torchvision.__version__))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sys.stdout.write('GPU mode \\n')\n",
    "else:\n",
    "    sys.stdout.write('Warning, CPU mode, pls check\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T11:44:37.559533Z",
     "start_time": "2019-05-02T11:44:37.554757Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.351191Z",
     "start_time": "2019-05-02T12:21:07.338669Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_loader(path, batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            #transforms.Resize(input_size),\n",
    "            #transforms.CenterCrop(input_size),\n",
    "            # use model fitted with the image size, so no need to resize\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.502, 0.474, 0.426], [0.227, 0.222, 0.226])\n",
    "            # https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "            # [mean],[std] for different channels\n",
    "        ]\n",
    "    )\n",
    "    sup_train_data = datasets.ImageFolder('{}/{}/train'.format(path, 'supervised'), transform=transform)\n",
    "    sup_val_data = datasets.ImageFolder('{}/{}/val'.format(path, 'supervised'), transform=transform)\n",
    "    unsup_data = datasets.ImageFolder('{}/{}/'.format(path, 'unsupervised'), transform=transform)\n",
    "    # source code: https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
    "    # Main idea:\n",
    "    data_loader_sup_train = torch.utils.data.DataLoader(\n",
    "        sup_train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_sup_val = torch.utils.data.DataLoader(\n",
    "        sup_val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_loader_unsup = torch.utils.data.DataLoader(\n",
    "        unsup_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print('sup_train_data.class_to_idx==sup_val_data.class_to_idx: ',\n",
    "          sup_train_data.class_to_idx == sup_val_data.class_to_idx)\n",
    "\n",
    "    return data_loader_sup_train, data_loader_sup_val, data_loader_unsup, sup_train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.358710Z",
     "start_time": "2019-05-02T12:21:07.354580Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_noise(img, noise_level):\n",
    "    noise = torch.randn(img.size()) * noise_level # randn produce rand num with mean 0 and var 1 (std 1 too)\n",
    "    noisy_img = img + noise\n",
    "    return noisy_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.367536Z",
     "start_time": "2019-05-02T12:21:07.362241Z"
    }
   },
   "outputs": [],
   "source": [
    "# used in initialize model\n",
    "def set_parameter_pin_grad(model, pinning):\n",
    "    if pinning:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.417896Z",
     "start_time": "2019-05-02T12:21:07.370591Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model_Based_Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, model_name, pretrained):\n",
    "        super(Model_Based_Autoencoder, self).__init__()\n",
    "        if model_name != 'vgg':\n",
    "            sys.stdout.write('Dear, we only support vgg now...')\n",
    "\n",
    "        self.encoder = models.vgg11_bn(pretrained=pretrained).features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),  # de-conv8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  # de-conv7\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),  # de-conv6\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  # de-conv5\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),  # de-conv4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  # de-conv3\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),  # de-conv2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),  # de-conv1\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.427167Z",
     "start_time": "2019-05-02T12:21:07.420515Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, pin_encoder, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "\n",
    "\n",
    "    if model_name != \"vgg\":\n",
    "        sys.stdout.write('We only have vgg now!!!')\n",
    "    else:\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = Model_Based_Autoencoder(model_name, pretrained=use_pretrained)\n",
    "        set_parameter_pin_grad(model_ft.encoder, pin_encoder)\n",
    "\n",
    "        input_size = 96\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.433628Z",
     "start_time": "2019-05-02T12:21:07.429371Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder_feature_hook(self,layer_input, layer_output):\n",
    "    encoder_feature_list.append(layer_output)\n",
    "def decoder_feature_hook(self,layer_input,layer_output):\n",
    "    decoder_feature_list.append(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:07.471005Z",
     "start_time": "2019-05-02T12:21:07.435873Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    \n",
    "    global encoder_feature_list\n",
    "    global decoder_feature_list\n",
    "    \n",
    "    # unsupervised learning, we do not need train and vals\n",
    "    since = time.time()\n",
    "    loss_history = []\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    encoder_layer_num_list=[3, 7, 14, 21] # maxpool outputs\n",
    "    decoder_layer_num_list=[5, 11, 17, 20] # ConvTrans strink input\n",
    "    \n",
    "    \n",
    "    for layer_num in encoder_layer_num_list:\n",
    "        #print(layer_num)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model.module.encoder[layer_num].register_forward_hook(encoder_feature_hook)\n",
    "        else:\n",
    "            model.encoder[layer_num].register_forward_hook(encoder_feature_hook)\n",
    "            #print(model.encoder[layer_num])\n",
    "\n",
    "    for layer_num in decoder_layer_num_list:\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model.module.decoder[layer_num].register_forward_hook(decoder_feature_hook)\n",
    "        else:\n",
    "            model.decoder[layer_num].register_forward_hook(decoder_feature_hook)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        ################# train the model on unsupervised data ############\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "\n",
    "        running_loss = 0.0\n",
    "        batch_counter=0\n",
    "        for inputs, _ in dataloaders['unlabeled']:\n",
    "            batch_counter+=1\n",
    "            print('batch_counter:',batch_counter)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            del encoder_feature_list[:] # layer input\n",
    "            del decoder_feature_list[:] # layer output\n",
    "            \n",
    "            print('length before forward:', len(encoder_feature_list))\n",
    "            print('length before forward:', len(decoder_feature_list))\n",
    "\n",
    "            \n",
    "\n",
    "            noisy_inputs = add_noise(inputs, noise_level)\n",
    "            inputs = inputs.to(device)\n",
    "            noisy_inputs = noisy_inputs.to(device)\n",
    "\n",
    "            outputs = model(noisy_inputs) # forward process over\n",
    "            \n",
    "            print('length after forward, en:', len(encoder_feature_list))\n",
    "            print('length after forward, de:', len(decoder_feature_list))\n",
    "\n",
    "            if torch.cuda.device_count() == 2:\n",
    "                decoder_feature_list[0:4]=decoder_feature_list[0:4][::-1]\n",
    "                decoder_feature_list[4:8] = decoder_feature_list[4:8][::-1]\n",
    "            else:\n",
    "                decoder_feature_list = decoder_feature_list[::-1]\n",
    "\n",
    "            # calculate stacked loss\n",
    "            loss=0\n",
    "            for encoder_feature, decoder_feature in zip(encoder_feature_list,decoder_feature_list):\n",
    "                loss += criterion(encoder_feature, decoder_feature)\n",
    "\n",
    "            ########################\n",
    "\n",
    "            i_o_loss = criterion(outputs, inputs)\n",
    "            running_loss += i_o_loss.item() * inputs.size(0)\n",
    "\n",
    "            loss+=i_o_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders['unlabeled'].dataset)\n",
    "        sys.stdout.write('Training time: {:.0f}s'.format(time.time() - since))\n",
    "        sys.stdout.write('Training loss: {:.4f}'.format(epoch_loss))\n",
    "        ################# evaluate the model performance on labeled data ############\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        eval_loss = 0.0\n",
    "        for inputs, _ in dataloaders['labeled']:\n",
    "            \n",
    "            del encoder_feature_list[:] # clean hooks\n",
    "            del decoder_feature_list[:] # clean hooks\n",
    "            print('val length before forward:', len(encoder_feature_list))\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            print('val length after forward:', len(encoder_feature_list))\n",
    "\n",
    "            loss = criterion(outputs, inputs)\n",
    "            eval_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_eval_loss = eval_loss / len(dataloaders['labeled'].dataset)\n",
    "        sys.stdout.write('Evaluation time: {:.0f}s'.format(time.time() - since))\n",
    "        sys.stdout.write(' Eval loss: {:.4f}'.format(epoch_eval_loss))\n",
    "\n",
    "        #################\n",
    "\n",
    "        loss_history.append((epoch_loss, epoch_eval_loss))\n",
    "\n",
    "        if epoch_eval_loss < best_loss:\n",
    "            best_loss = epoch_eval_loss\n",
    "            #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            with open(save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "\n",
    "        with open(save_path + '_loss', 'w') as f:\n",
    "            for item in loss_history:\n",
    "                f.write(\"unlabeled: %s, labeled: %s \\n,  \" % (item[0], item[1]))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:11.770544Z",
     "start_time": "2019-05-02T12:21:07.473363Z"
    }
   },
   "outputs": [],
   "source": [
    "####### make sure model and input size\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, pin_encoder, use_pretrained=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_ft = nn.DataParallel(model_ft)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:18.273419Z",
     "start_time": "2019-05-02T12:21:11.773488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin to load data...\n",
      "sup_train_data.class_to_idx==sup_val_data.class_to_idx:  True\n"
     ]
    }
   ],
   "source": [
    "####### load data, input_size is used ####\n",
    "\n",
    "sys.stdout.write('\\nBegin to load data...\\n')\n",
    "\n",
    "dataloaders={}\n",
    "\n",
    "dataloaders['labeled'], data_loader_val, dataloaders['unlabeled'], class_to_idx_dict = image_loader(loader_image_path,loader_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:18.293378Z",
     "start_time": "2019-05-02T12:21:18.275660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\tmodule.encoder.0.weight\n",
      "\tmodule.encoder.0.bias\n",
      "\tmodule.encoder.1.weight\n",
      "\tmodule.encoder.1.bias\n",
      "\tmodule.encoder.4.weight\n",
      "\tmodule.encoder.4.bias\n",
      "\tmodule.encoder.5.weight\n",
      "\tmodule.encoder.5.bias\n",
      "\tmodule.encoder.8.weight\n",
      "\tmodule.encoder.8.bias\n",
      "\tmodule.encoder.9.weight\n",
      "\tmodule.encoder.9.bias\n",
      "\tmodule.encoder.11.weight\n",
      "\tmodule.encoder.11.bias\n",
      "\tmodule.encoder.12.weight\n",
      "\tmodule.encoder.12.bias\n",
      "\tmodule.encoder.15.weight\n",
      "\tmodule.encoder.15.bias\n",
      "\tmodule.encoder.16.weight\n",
      "\tmodule.encoder.16.bias\n",
      "\tmodule.encoder.18.weight\n",
      "\tmodule.encoder.18.bias\n",
      "\tmodule.encoder.19.weight\n",
      "\tmodule.encoder.19.bias\n",
      "\tmodule.encoder.22.weight\n",
      "\tmodule.encoder.22.bias\n",
      "\tmodule.encoder.23.weight\n",
      "\tmodule.encoder.23.bias\n",
      "\tmodule.encoder.25.weight\n",
      "\tmodule.encoder.25.bias\n",
      "\tmodule.encoder.26.weight\n",
      "\tmodule.encoder.26.bias\n",
      "\tmodule.decoder.0.weight\n",
      "\tmodule.decoder.0.bias\n",
      "\tmodule.decoder.1.weight\n",
      "\tmodule.decoder.1.bias\n",
      "\tmodule.decoder.3.weight\n",
      "\tmodule.decoder.3.bias\n",
      "\tmodule.decoder.4.weight\n",
      "\tmodule.decoder.4.bias\n",
      "\tmodule.decoder.6.weight\n",
      "\tmodule.decoder.6.bias\n",
      "\tmodule.decoder.7.weight\n",
      "\tmodule.decoder.7.bias\n",
      "\tmodule.decoder.9.weight\n",
      "\tmodule.decoder.9.bias\n",
      "\tmodule.decoder.10.weight\n",
      "\tmodule.decoder.10.bias\n",
      "\tmodule.decoder.12.weight\n",
      "\tmodule.decoder.12.bias\n",
      "\tmodule.decoder.13.weight\n",
      "\tmodule.decoder.13.bias\n",
      "\tmodule.decoder.15.weight\n",
      "\tmodule.decoder.15.bias\n",
      "\tmodule.decoder.16.weight\n",
      "\tmodule.decoder.16.bias\n",
      "\tmodule.decoder.18.weight\n",
      "\tmodule.decoder.18.bias\n",
      "\tmodule.decoder.19.weight\n",
      "\tmodule.decoder.19.bias\n",
      "\tmodule.decoder.21.weight\n",
      "\tmodule.decoder.21.bias\n",
      "\tmodule.decoder.22.weight\n",
      "\tmodule.decoder.22.bias\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if pin_encoder:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            sys.stdout.write(\"\\t{}\\n\".format(name))\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            sys.stdout.write(\"\\t{}\\n\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:27.993965Z",
     "start_time": "2019-05-02T12:21:18.295484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to train...Epoch 0/39\n",
      "----------\n",
      "batch_counter: 1\n",
      "length before forward: 0\n",
      "length before forward: 0\n",
      "length after forward, en: 8\n",
      "length after forward, de: 8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (48) must match the size of tensor b (24) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-832810192f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-08ab6fb4f57c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mencoder_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_feature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_feature_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_feature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, reduction)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'elementwise_mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (48) must match the size of tensor b (24) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "######### Observe that all parameters are being optimized\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate=0.001\n",
    "optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Setup the loss fxn\n",
    "\n",
    "sys.stdout.write('Begin to train...')\n",
    "\n",
    "encoder_feature_list=[] # layer input\n",
    "decoder_feature_list=[]\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)\n",
    "\n",
    "sys.stdout.write('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:38.219877Z",
     "start_time": "2019-05-02T12:21:38.214876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:21:38.477293Z",
     "start_time": "2019-05-02T12:21:38.472958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:22:07.506898Z",
     "start_time": "2019-05-02T12:22:07.501365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64, 48, 48])\n",
      "torch.Size([128, 64, 48, 48])\n",
      "torch.Size([128, 128, 24, 24])\n",
      "torch.Size([128, 128, 24, 24])\n",
      "torch.Size([128, 256, 12, 12])\n",
      "torch.Size([128, 256, 12, 12])\n",
      "torch.Size([128, 512, 6, 6])\n",
      "torch.Size([128, 512, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "for feature in encoder_feature_list:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:22:08.096151Z",
     "start_time": "2019-05-02T12:22:08.091123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 24, 24])\n",
      "torch.Size([128, 512, 6, 6])\n",
      "torch.Size([128, 256, 12, 12])\n",
      "torch.Size([128, 512, 6, 6])\n",
      "torch.Size([128, 64, 48, 48])\n",
      "torch.Size([128, 128, 24, 24])\n",
      "torch.Size([128, 256, 12, 12])\n",
      "torch.Size([128, 64, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "for feature in decoder_feature_list:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:25:10.679900Z",
     "start_time": "2019-05-02T12:25:10.675635Z"
    }
   },
   "outputs": [],
   "source": [
    "lst = [1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:25:10.950712Z",
     "start_time": "2019-05-02T12:25:10.946126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:25:11.166455Z",
     "start_time": "2019-05-02T12:25:11.161926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:25:11.525078Z",
     "start_time": "2019-05-02T12:25:11.521012Z"
    }
   },
   "outputs": [],
   "source": [
    "lst[0:4]=lst[0:4][::-1]\n",
    "lst[4:8]=lst[4:8][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T12:25:16.452969Z",
     "start_time": "2019-05-02T12:25:16.448351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 2, 1, 8, 7, 6, 5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
