begin_dn_vggae_fromscratch_model
PyTorch Version: 0.4.1Torchvision Version: GPU modeBegin to load data...sup_train_data.class_to_idx==sup_val_data.class_to_idx:  True
Params to learn:
	module.encoder.0.weight	module.encoder.0.bias	module.encoder.1.weight	module.encoder.1.bias	module.encoder.4.weight	module.encoder.4.bias	module.encoder.5.weight	module.encoder.5.bias	module.encoder.8.weight	module.encoder.8.bias	module.encoder.9.weight	module.encoder.9.bias	module.encoder.11.weight	module.encoder.11.bias	module.encoder.12.weight	module.encoder.12.bias	module.encoder.15.weight	module.encoder.15.bias	module.encoder.16.weight	module.encoder.16.bias	module.encoder.18.weight	module.encoder.18.bias	module.encoder.19.weight	module.encoder.19.bias	module.encoder.22.weight	module.encoder.22.bias	module.encoder.23.weight	module.encoder.23.bias	module.encoder.25.weight	module.encoder.25.bias	module.encoder.26.weight	module.encoder.26.bias	module.decoder.0.weight	module.decoder.0.bias	module.decoder.1.weight	module.decoder.1.bias	module.decoder.3.weight	module.decoder.3.bias	module.decoder.4.weight	module.decoder.4.bias	module.decoder.6.weight	module.decoder.6.bias	module.decoder.7.weight	module.decoder.7.bias	module.decoder.9.weight	module.decoder.9.bias	module.decoder.10.weight	module.decoder.10.bias	module.decoder.12.weight	module.decoder.12.bias	module.decoder.13.weight	module.decoder.13.bias	module.decoder.15.weight	module.decoder.15.bias	module.decoder.16.weight	module.decoder.16.bias	module.decoder.18.weight	module.decoder.18.bias	module.decoder.19.weight	module.decoder.19.bias	module.decoder.21.weight	module.decoder.21.bias	module.decoder.22.weight	module.decoder.22.biasBegin to train...Epoch 0/39
----------
Training time: 14359sTraining loss: 0.7877Traceback (most recent call last):
  File "190427_main_dn_vggae.py", line 332, in <module>
    model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)
  File "190427_main_dn_vggae.py", line 248, in train_model
    outputs = model(inputs)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 123, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 133, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    raise output
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 53, in _worker
    output = module(*input, **kwargs)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "190427_main_dn_vggae.py", line 177, in forward
    x = self.encoder(x)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/by783/myenv/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA error: out of memory
slurmstepd: error: The SLURM utilizes Linux cgroups for resource containment. For reference please read the explanation in the bug report: https://bugs.schedmd.com/show_bug.cgi?id=3214#c4
slurmstepd: error: The SLURM utilizes Linux cgroups for resource containment. For reference please read the explanation in the bug report: https://bugs.schedmd.com/show_bug.cgi?id=3214#c4
